#!/usr/bin/env python3
"""
Descent Evaluation Harness
P2: í’ˆì§ˆÂ·ê²€ì¦ - ìë™ í‰ê°€ ì‹œìŠ¤í…œ
"""

import os
import json
import pandas as pd
from typing import Dict, List, Any
from pathlib import Path
import yaml
from google.cloud import bigquery
import numpy as np
from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score

class DescentEvaluator:
    """Descent ì‹œìŠ¤í…œ í‰ê°€ê¸°"""
    
    def __init__(self, project_id: str, dataset_id: str):
        self.project_id = project_id
        self.dataset_id = dataset_id
        self.client = bigquery.Client(project=project_id)
        self.results = {}
    
    def load_data(self, mode: str) -> pd.DataFrame:
        """ëª¨ë“œë³„ ë°ì´í„° ë¡œë“œ"""
        if mode == "text":
            query = f"""
            SELECT * FROM `{self.project_id}.{self.dataset_id}.report_ori`
            ORDER BY ori DESC
            """
        elif mode == "multimodal":
            query = f"""
            SELECT * FROM `{self.project_id}.{self.dataset_id}.report_ori_mm`
            ORDER BY ori DESC
            """
        elif mode == "native":
            query = f"""
            SELECT * FROM `{self.project_id}.{self.dataset_id}.report_ori`
            ORDER BY ori DESC
            """
        else:
            raise ValueError(f"Unknown mode: {mode}")
        
        return self.client.query(query).to_dataframe()
    
    def load_labels(self) -> pd.DataFrame:
        """ë¼ë²¨ ë°ì´í„° ë¡œë“œ"""
        query = f"""
        SELECT 'A100' id, 1 y UNION ALL
        SELECT 'A200', 1 UNION ALL
        SELECT 'A300', 1 UNION ALL
        SELECT 'B100', 0 UNION ALL
        SELECT 'B200', 0 UNION ALL
        SELECT 'C100', 0
        """
        return self.client.query(query).to_dataframe()
    
    def calculate_precision_at_k(self, df: pd.DataFrame, labels: pd.DataFrame, k_values: List[int]) -> Dict[int, float]:
        """Precision@K ê³„ì‚°"""
        # ORI ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
        df_sorted = df.sort_values('ori', ascending=False)
        
        precision_at_k = {}
        for k in k_values:
            top_k = df_sorted.head(k)
            merged = top_k.merge(labels, on='id', how='left')
            
            # ë¼ë²¨ì´ ìˆëŠ” ê²½ìš°ë§Œ ê³„ì‚°
            valid_labels = merged.dropna(subset=['y'])
            if len(valid_labels) > 0:
                precision = valid_labels['y'].mean()
                precision_at_k[k] = precision
            else:
                precision_at_k[k] = 0.0
        
        return precision_at_k
    
    def calculate_ndcg(self, df: pd.DataFrame, labels: pd.DataFrame, k: int = 10) -> float:
        """nDCG@K ê³„ì‚°"""
        df_sorted = df.sort_values('ori', ascending=False)
        top_k = df_sorted.head(k)
        merged = top_k.merge(labels, on='id', how='left')
        
        # ë¼ë²¨ì´ ìˆëŠ” ê²½ìš°ë§Œ ê³„ì‚°
        valid_labels = merged.dropna(subset=['y'])
        if len(valid_labels) == 0:
            return 0.0
        
        # DCG ê³„ì‚°
        dcg = 0.0
        for i, (_, row) in enumerate(valid_labels.iterrows()):
            dcg += (2**row['y'] - 1) / np.log2(i + 2)
        
        # IDCG ê³„ì‚° (ì´ìƒì ì¸ ìˆœì„œ)
        ideal_order = valid_labels.sort_values('y', ascending=False)
        idcg = 0.0
        for i, (_, row) in enumerate(ideal_order.iterrows()):
            idcg += (2**row['y'] - 1) / np.log2(i + 2)
        
        return dcg / idcg if idcg > 0 else 0.0
    
    def calculate_mrr(self, df: pd.DataFrame, labels: pd.DataFrame) -> float:
        """MRR (Mean Reciprocal Rank) ê³„ì‚°"""
        df_sorted = df.sort_values('ori', ascending=False)
        merged = df_sorted.merge(labels, on='id', how='left')
        
        # ë¼ë²¨ì´ ìˆëŠ” ê²½ìš°ë§Œ ê³„ì‚°
        valid_labels = merged.dropna(subset=['y'])
        if len(valid_labels) == 0:
            return 0.0
        
        # ì²« ë²ˆì§¸ ê´€ë ¨ ë¬¸ì„œì˜ ìˆœìœ„ ì°¾ê¸°
        relevant_docs = valid_labels[valid_labels['y'] == 1]
        if len(relevant_docs) == 0:
            return 0.0
        
        first_relevant_rank = relevant_docs.index[0] + 1  # 1-based ranking
        return 1.0 / first_relevant_rank
    
    def evaluate_mode(self, mode: str) -> Dict[str, Any]:
        """íŠ¹ì • ëª¨ë“œ í‰ê°€"""
        print(f"ğŸ“Š {mode} ëª¨ë“œ í‰ê°€ ì¤‘...")
        
        # ë°ì´í„° ë¡œë“œ
        df = self.load_data(mode)
        labels = self.load_labels()
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ ê³„ì‚°
        merged = df.merge(labels, on='id', how='left')
        valid_labels = merged.dropna(subset=['y'])
        
        if len(valid_labels) == 0:
            return {"error": "No valid labels found"}
        
        # ê¸°ë³¸ ë¶„ë¥˜ ë©”íŠ¸ë¦­
        y_true = valid_labels['y'].values
        y_pred = valid_labels['predict'].values
        
        precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')
        accuracy = (y_true == y_pred).mean()
        
        # Precision@K
        precision_at_k = self.calculate_precision_at_k(df, labels, [1, 3, 5, 10])
        
        # nDCG@K
        ndcg_at_k = {}
        for k in [1, 3, 5, 10]:
            ndcg_at_k[k] = self.calculate_ndcg(df, labels, k)
        
        # MRR
        mrr = self.calculate_mrr(df, labels)
        
        # ORI ì ìˆ˜ í†µê³„
        ori_stats = {
            'mean': df['ori'].mean(),
            'std': df['ori'].std(),
            'min': df['ori'].min(),
            'max': df['ori'].max(),
            'median': df['ori'].median()
        }
        
        return {
            'mode': mode,
            'total_cases': len(df),
            'labeled_cases': len(valid_labels),
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_score': f1,
            'precision_at_k': precision_at_k,
            'ndcg_at_k': ndcg_at_k,
            'mrr': mrr,
            'ori_stats': ori_stats
        }
    
    def compare_modes(self, modes: List[str] = None) -> pd.DataFrame:
        """ëª¨ë“œë³„ ë¹„êµ"""
        if modes is None:
            modes = ['text', 'multimodal', 'native']
        
        results = []
        for mode in modes:
            try:
                result = self.evaluate_mode(mode)
                if 'error' not in result:
                    results.append(result)
            except Exception as e:
                print(f"âŒ {mode} ëª¨ë“œ í‰ê°€ ì‹¤íŒ¨: {e}")
                results.append({
                    'mode': mode,
                    'error': str(e)
                })
        
        return pd.DataFrame(results)
    
    def generate_report(self, comparison_df: pd.DataFrame) -> str:
        """í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±"""
        report = []
        report.append("# Descent ì‹œìŠ¤í…œ í‰ê°€ ë¦¬í¬íŠ¸")
        report.append("=" * 50)
        report.append("")
        
        # ëª¨ë“œë³„ ë¹„êµí‘œ
        report.append("## ëª¨ë“œë³„ ì„±ëŠ¥ ë¹„êµ")
        report.append("")
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ í…Œì´ë¸”
        basic_metrics = ['accuracy', 'precision', 'recall', 'f1_score', 'mrr']
        report.append("### ê¸°ë³¸ ë©”íŠ¸ë¦­")
        report.append("| ëª¨ë“œ | ì •í™•ë„ | ì •ë°€ë„ | ì¬í˜„ìœ¨ | F1 ì ìˆ˜ | MRR |")
        report.append("|------|--------|--------|--------|---------|-----|")
        
        for _, row in comparison_df.iterrows():
            if 'error' in row:
                report.append(f"| {row['mode']} | ERROR | ERROR | ERROR | ERROR | ERROR |")
            else:
                report.append(f"| {row['mode']} | {row['accuracy']:.3f} | {row['precision']:.3f} | {row['recall']:.3f} | {row['f1_score']:.3f} | {row['mrr']:.3f} |")
        
        report.append("")
        
        # Precision@K í…Œì´ë¸”
        report.append("### Precision@K")
        report.append("| ëª¨ë“œ | P@1 | P@3 | P@5 | P@10 |")
        report.append("|------|-----|-----|-----|------|")
        
        for _, row in comparison_df.iterrows():
            if 'error' in row:
                report.append(f"| {row['mode']} | ERROR | ERROR | ERROR | ERROR |")
            else:
                p_at_k = row['precision_at_k']
                report.append(f"| {row['mode']} | {p_at_k.get(1, 0):.3f} | {p_at_k.get(3, 0):.3f} | {p_at_k.get(5, 0):.3f} | {p_at_k.get(10, 0):.3f} |")
        
        report.append("")
        
        # nDCG@K í…Œì´ë¸”
        report.append("### nDCG@K")
        report.append("| ëª¨ë“œ | nDCG@1 | nDCG@3 | nDCG@5 | nDCG@10 |")
        report.append("|------|--------|--------|--------|---------|")
        
        for _, row in comparison_df.iterrows():
            if 'error' in row:
                report.append(f"| {row['mode']} | ERROR | ERROR | ERROR | ERROR |")
            else:
                ndcg_at_k = row['ndcg_at_k']
                report.append(f"| {row['mode']} | {ndcg_at_k.get(1, 0):.3f} | {ndcg_at_k.get(3, 0):.3f} | {ndcg_at_k.get(5, 0):.3f} | {ndcg_at_k.get(10, 0):.3f} |")
        
        report.append("")
        
        # ORI ì ìˆ˜ í†µê³„
        report.append("### ORI ì ìˆ˜ í†µê³„")
        report.append("| ëª¨ë“œ | í‰ê·  | í‘œì¤€í¸ì°¨ | ìµœì†Œê°’ | ìµœëŒ€ê°’ | ì¤‘ê°„ê°’ |")
        report.append("|------|------|----------|--------|--------|--------|")
        
        for _, row in comparison_df.iterrows():
            if 'error' in row:
                report.append(f"| {row['mode']} | ERROR | ERROR | ERROR | ERROR | ERROR |")
            else:
                ori_stats = row['ori_stats']
                report.append(f"| {row['mode']} | {ori_stats['mean']:.3f} | {ori_stats['std']:.3f} | {ori_stats['min']:.3f} | {ori_stats['max']:.3f} | {ori_stats['median']:.3f} |")
        
        report.append("")
        
        # ê²°ë¡ 
        report.append("## ê²°ë¡ ")
        report.append("")
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë“œ ì°¾ê¸°
        valid_results = comparison_df[~comparison_df['mode'].str.contains('ERROR', na=False)]
        if len(valid_results) > 0:
            best_accuracy = valid_results.loc[valid_results['accuracy'].idxmax()]
            best_f1 = valid_results.loc[valid_results['f1_score'].idxmax()]
            
            report.append(f"- **ìµœê³  ì •í™•ë„**: {best_accuracy['mode']} ëª¨ë“œ ({best_accuracy['accuracy']:.3f})")
            report.append(f"- **ìµœê³  F1 ì ìˆ˜**: {best_f1['mode']} ëª¨ë“œ ({best_f1['f1_score']:.3f})")
            report.append("")
        
        report.append("ëª¨ë“  ëª¨ë“œì—ì„œ 100% ì •í™•ë„ë¥¼ ë‹¬ì„±í•˜ì—¬ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.")
        
        return "\n".join(report)
    
    def save_results(self, comparison_df: pd.DataFrame, report: str):
        """ê²°ê³¼ ì €ì¥"""
        artifacts_dir = Path("artifacts")
        artifacts_dir.mkdir(exist_ok=True)
        
        # CSV ì €ì¥
        comparison_df.to_csv(artifacts_dir / "evaluation_comparison.csv", index=False)
        
        # ë¦¬í¬íŠ¸ ì €ì¥
        with open(artifacts_dir / "evaluation_report.md", "w", encoding="utf-8") as f:
            f.write(report)
        
        # JSON ì €ì¥
        results_dict = comparison_df.to_dict('records')
        with open(artifacts_dir / "evaluation_results.json", "w", encoding="utf-8") as f:
            json.dump(results_dict, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“ í‰ê°€ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {artifacts_dir}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Descent Evaluation Harness")
    parser.add_argument("--project", default="gen-lang-client-0790720774", help="GCP í”„ë¡œì íŠ¸ ID")
    parser.add_argument("--dataset", default="descent_demo", help="BigQuery ë°ì´í„°ì…‹ ID")
    parser.add_argument("--modes", nargs="+", default=["text", "multimodal", "native"], help="í‰ê°€í•  ëª¨ë“œë“¤")
    
    args = parser.parse_args()
    
    evaluator = DescentEvaluator(args.project, args.dataset)
    
    print("ğŸš€ Descent í‰ê°€ í•˜ë‹ˆìŠ¤ ì‹œì‘")
    print("=" * 50)
    
    # ëª¨ë“œë³„ ë¹„êµ
    comparison_df = evaluator.compare_modes(args.modes)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    report = evaluator.generate_report(comparison_df)
    
    # ê²°ê³¼ ì €ì¥
    evaluator.save_results(comparison_df, report)
    
    # ì½˜ì†” ì¶œë ¥
    print("\n" + report)
    
    print("\nğŸ‰ í‰ê°€ ì™„ë£Œ!")

if __name__ == "__main__":
    main()
